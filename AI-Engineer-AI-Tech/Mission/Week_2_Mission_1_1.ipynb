{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### [TODO] 코드 구현 : 모델 예측 결과 csv로 만들기"
      ],
      "metadata": {
        "id": "nnlHdawSH6i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "!wget –no-check-certificate 'https://docs.google.com/uc?export=download&id=1IVvuG3SMlarSSGmcliGFjq1fMxZtksE0' -O kaggle-kakr-housing-data.zip\n",
        "\n",
        "# 다운로드 받은 zip파일 압축풀기\n",
        "!unzip -qq ./kaggle-kakr-housing-data.zip\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "train_data_path = join('./data/train.csv')\n",
        "sub_data_path = join('./data/test.csv')      # 테스트, 즉 submission 시 사용할 데이터 경로\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "data = pd.read_csv(train_data_path)\n",
        "sub = pd.read_csv(sub_data_path)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "y = data['price']\n",
        "del data['price']\n",
        "\n",
        "train_len = len(data)\n",
        "data = pd.concat((data, sub), axis=0)\n",
        "\n",
        "sub_id = data['id'][train_len:]\n",
        "del data['id']\n",
        "\n",
        "data['date'] = data['date'].apply(lambda x : str(x[:6])).astype(int)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\n",
        "\n",
        "for c in skew_columns:\n",
        "    data[c] = np.log1p(data[c].values)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "y_log_transformation = np.log1p(y)\n",
        "\n",
        "sub = data.iloc[train_len:, :] # 테스트 데이터\n",
        "x = data.iloc[:train_len, :] # 학습데이터\n",
        "\n",
        "print(x.shape)\n",
        "print(sub.shape)\n",
        "\n",
        "gboost = GradientBoostingRegressor(random_state=2023)\n",
        "xgboost = xgb.XGBRegressor(random_state=2023)\n",
        "lightgbm = lgb.LGBMRegressor(random_state=2023)\n",
        "\n",
        "models = [{'model':gboost, 'name':'GradientBoosting'}, {'model':xgboost, 'name':'XGBoost'},\n",
        "          {'model':lightgbm, 'name':'LightGBM'}]\n",
        "\n",
        "def get_cv_score(models):\n",
        "    kfold = KFold(n_splits=5).get_n_splits(x.values)\n",
        "    for m in models:\n",
        "        CV_score = np.mean(cross_val_score(m['model'], X=x.values, y=y, cv=kfold))\n",
        "        print(f\"Model: {m['name']}, CV score:{CV_score:.4f}\")\n",
        "\n",
        "get_cv_score(models)\n",
        "\n",
        "def AveragingBlending(models, x, y, sub_x):\n",
        "    # 모델학습\n",
        "    for m in models :\n",
        "        m['model'].fit(x.values, y)\n",
        "\n",
        "    # 모델예측\n",
        "    predictions = np.column_stack([\n",
        "        m['model'].predict(sub_x.values) for m in models\n",
        "    ])\n",
        "\n",
        "    # 각 모델 에측의 평균을 return\n",
        "    return np.mean(predictions, axis=1)\n",
        "\n",
        "y_pred = AveragingBlending(models, x, y, sub)\n",
        "print(len(y_pred))\n",
        "y_pred\n",
        "\n",
        "submission = pd.read_csv('./data/sample_submission.csv')\n",
        "submission.head()\n",
        "\n",
        "result = pd.DataFrame({\n",
        "    'id' : sub_id,\n",
        "    'price' : y_pred\n",
        "})\n",
        "\n",
        "result.head()\n",
        "\n",
        "my_submission_path = './data/submission.csv'\n",
        "\n",
        "## 코드시작 ##\n",
        "# 미션 코드 작성 : result를 'my_submission_path'란 이름의 csv로 저장해 주세요.\n",
        "\n",
        "result.to_csv(my_submission_path)\n",
        "\n",
        "## 코드종료 ##"
      ],
      "metadata": {
        "id": "zTViIaMwJ5KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9954d3-48a8-41fe-9ba4-cdca13216bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 07:55:55--  http://xn--no-check-certificate-2t2l/\n",
            "Resolving xn--no-check-certificate-2t2l (xn--no-check-certificate-2t2l)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘xn--no-check-certificate-2t2l’\n",
            "--2024-01-24 07:55:55--  https://docs.google.com/uc?export=download&id=1IVvuG3SMlarSSGmcliGFjq1fMxZtksE0\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.202.138, 173.194.202.139, 173.194.202.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.202.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1IVvuG3SMlarSSGmcliGFjq1fMxZtksE0&export=download [following]\n",
            "--2024-01-24 07:55:56--  https://drive.usercontent.google.com/download?id=1IVvuG3SMlarSSGmcliGFjq1fMxZtksE0&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.135.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.135.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 693719 (677K) [application/octet-stream]\n",
            "Saving to: ‘kaggle-kakr-housing-data.zip’\n",
            "\n",
            "kaggle-kakr-housing 100%[===================>] 677.46K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2024-01-24 07:55:57 (93.4 MB/s) - ‘kaggle-kakr-housing-data.zip’ saved [693719/693719]\n",
            "\n",
            "FINISHED --2024-01-24 07:55:57--\n",
            "Total wall clock time: 1.3s\n",
            "Downloaded: 1 files, 677K in 0.007s (93.4 MB/s)\n",
            "(15035, 19)\n",
            "(6468, 19)\n",
            "Model: GradientBoosting, CV score:0.8609\n",
            "Model: XGBoost, CV score:0.8763\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001293 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2296\n",
            "[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score 540497.991270\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001263 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2327\n",
            "[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score 542956.681826\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2331\n",
            "[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score 543149.529265\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001228 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2332\n",
            "[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score 542032.619305\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2298\n",
            "[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score 534776.444047\n",
            "Model: LightGBM, CV score:0.8819\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2338\n",
            "[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score 540682.653143\n",
            "6468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TODO] 코드 구현 : public score 확인하고, 제출 결과 공유하기"
      ],
      "metadata": {
        "id": "k8uJA-FdIAeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 코드시작 ##\n",
        "# 미션 코드 작성 : 캐글에 결과를 제출하고, Public Score를 확인해보세요.\n",
        "print(\"나의 Public Score 점수는 : \")\n",
        "print(\"112690.93543\")  # ... 에 점수를 기록해주세요.\n",
        "\n",
        "## 코드종료 ##"
      ],
      "metadata": {
        "id": "jYk87jGXJ75Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35420bf-0269-4ac7-8a7a-0d66bd71b858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나의 Public Score 점수는 : \n",
            "112690.93543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALL RIGHTS RESERVED. (C)NAVER Connect Foundation."
      ],
      "metadata": {
        "id": "L2qw-syrIMhv"
      }
    }
  ]
}